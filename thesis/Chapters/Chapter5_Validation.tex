\chapter{Validation}

  This chapter will go through all the validation and tests implemented to measure the performance and the correct functioning of the new behaviors. 

  To validate each implemented behavior we propose using them both in simulation and in real flight missions. The validation tests are presented in detail in section \ref{ch_5:sect:val_tests}, explaining what the tests measure and how they do it as well as the intuition behind each test. Afterwards, in section \ref{ch_5:sect:experiments} we explain in detail the mission implemented for each behavior along with further information of the simulator employed in section \ref{ch_5:subsect:exp_simulation}. The simulation integration with the current version of Aerostack and the specific details of the real flight aircraft are detailed in section \ref{ch_5:subsect:exp_real_flight}. Section \ref{ch_5:sect:results} reports on the results obtained for each mission both in sumulation and in real flight and \ref{ch_5:sect:discussion} closes the chapter with the discussion of the results.
 
\section{Validation Tests} \label{ch_5:sect:val_tests}

  A validation test should ensure that the behavior complies with the following constraints:

  \begin{enumerate}
    \item \underline{Functioning}: It should do what it is supposed to do. No more, no less.
    \item \underline{Requirements}: It should meet the imposed requirements (explained in \ref{ch_4:sect:requirements})
  \end{enumerate}

  The first constraint is the most obvious, a behavior is designed to do a concrete task. It should do only what is designed to do. The idea behind a behavior is to encapsulate a concrete algorithm, it can be the layer that encapsulates the algorithm, providing a standard API access, but it can only be one functionality.

  To test this constraint we will provide both a simulation and a real mission and put to work each behavior independently, testing that each one works as expected. We will measure the precision of each behavior based on the amount of the task that it is able to complete without errors.

  One of the hardest requirements to meet is that of the efficiency, any implemented behavior should be efficient enough to run both onboard and in ground control stations in real time. Although this requirement could not be a problem for a PC it surely can be a problem on onborad computers. Also, it is difficult to measure for real flight missions because a cpu tracker also takes computing resources and the onboard computers mounted on the UAVs are not very powerful. Nevertheless, the cpu usage will be measured both in sumulation and in real flight and the corresponding data will be provided for analysis.

\section{Simulation} \label{ch_5:sect:simulation}

  The chosen simulator for these tests is Gazebo Sim (\cite{gazebo_web}), an open source, multiplatform, robot simulator. It was chosen because of it's open source nature, ease of use and great integration with ROS. Gazebo features an open, modular, plugin based architecture, which makes it perfect to integrate new components and opens the door for modules being simulated inside it and the outside world. In our case, to communicate the simulated UAV with ROS and the Aerostack framework.

  In order to convey the simulator with ROS we will employ an already made plugin called RotorS \cite{rotors2016}. RotorS provides some UAV models and a plugin that translates gazebo topics to ROS ones, unifying the access to the data. This architecture is perfect for any robotic environment as it minimizes the overhead of changing from simulation to real flight, as long as the topics are called the same, the Aerostack framework does not even notice it.

  As all the simulation is launched with configuration files and was done with flexibility in mind, we could adapt the already available configurations to our needs, minimizing the overhead of naming topics to our conventions.

  To choose a UAV model from the available ones we looked up for one that matches our requirements, namely a lidar sensor (hokuyo if possible), a front camera and an altitude sensor. RotorS provides a UAV modeled after the AscTec Hummingbird \cite{hummingbird_web} drone, which meets all these requirements.

\section{Real Flight} \label{ch_5:sect:real_flight}

  Our requirements came mainly because of the implemented behaviors, which in turn came from the available hardware in the research group. Currently, the aircraft used for the most important missions is a Matrice 100 from DJI (\cite{dji_matrice_web}) with all the sensorization cited above.

  This is used because of it's extendability (any sensor platform can be plugged in), open API and powerful motors. It's verstaile enough to provide a testbed for many research experiments and the battery lasts enough for medium duration missions.

\section{Testing Mission} \label{ch_5:sect:testing_mission}

  To provide the most realistic environment possible, the mission used to conduct all the experiments will be based in a real assignment requested to the research group a few months ago. The mission consists in inspecting the internal facade of a plant's boiler. At the time of request this new navigation interface was not implemented, so the flight was made almost by hand. In fact, this inteface was proposed after the need of an autonomous flight navigator whith the available hardware.

  The idea behind the mission is to fly a drone along the facade filming all the breathers, after the mission is completed, the film is extracted and given to the plant experts for their analysis. Additionally, a handmade mission could be very costly for the gas company as they would have had to install a portable crane and put a human to do the inspection of the 40 meters long facade.

  A human commanded drone accomplished the whole inspection in less than one hour. Furthermore, even in that case it was challenging for the human operator as he had to fly near the wall, which causes the drone to destabilize due to the air flows. This is the point where autonomous navigation comes in. In an autonomous mission, the control loop can be closed with any parameter that can be measured, in this case, the go to point behavior could have been employed to make the aircraft fly upwards until the whole boiler is inspected and then commanded back to land at a certain point.

  As the job is already done it is not possible to replicate it, hence we propose to simulate a boiler in Gazebo that resembles to the real mission and the inspection of an internal facade in a building for the real flight.

  The next sections will deepen in all the details of the conduted experiments and the results obtained during the tests.

\section{Experiments} \label{ch_5:sect:experiments}

  As mentioned before, for each behavior we will ensure it complies with our constraints: correct functioning and requirements (efficiency mostly).
  
  Once the whole Aerostack system is deployed, the first test for each behavior consists in checking it's activation conditions, i.e.: the \textit{behavior self localize and map by lidar} cannot work without a lidar, deploying the whole Aerostack in a lidar-less UAV should cause the behavior to be deactivated instantly.

  This way we tested all conditions and capabilities of all the proposed behaviors. As this is all software related, we easily corrected all the bugs found. One example of this was the access to the path planner module: The new planner (\textit{move base}) can only plan to one goal, this makes all the behaviors requiring this module mutually incompatible, namely, the behavior that generates paths cannot work simultaneously with that of the go to point . In the same fashion the behaviors that make use of the trajectory planner cannot work together or a collision could occur.

  For the functioning constraint we setup both the real and simulated missions with a python script that commands the Aerostack, if after doing a certain amount of missions, the behaviors work as expected we consider that they work correctly. 
  
  The following subsections explain the implemented mission for each behavior. Note that, the first two missions where only tested on the simulated environment to minimize the human operator time needed for the tests. For all missions both the simulation and the real environments used are the same, nothing was changed in the environment.

  \subsection{Behavior Generate Path in Occupancy Grid} \label{ch_5:subsect:behav_genpath_mission}

    This mission was only tested in simulation, it consisted in generating a path for every point in the go to point mission (sect. \ref{ch_5:subsect:behav_gtp_mission}). After the mission is finished, $6$ paths should be present in the belief memory. Therefore the mission can be described as follows:
    
    \begin{enumerate}
      \item Generate path for point: [0, 0, 1.5]
      \item Generate path for point: [1, 0, 1.5]
      \item Generate path for point: [1, 0, 10]
      \item Generate path for point: [1, -5, 10]
      \item Generate path for point: [0, -5, 1.5]
      \item Generate path for point: [0, -5, 1]
    \end{enumerate}

    The full python source code can be found in the appendix \ref{app1:generate_path_mission}

  \subsection{Behavior Follow Path in Occupancy Grid} \label{ch_5:subsect:behav_fpath_mission}

  In this mission we employ the previous behavior to generate paths from the current position to the target point and fed it as input parameter to this behavior. Working in tandem, these behaviors do a similar job to the go to point one (without the obstacle avoidance). The generated mission is outlined as:

    \begin{enumerate}
      \item Follow path for point: [0, 0, 1.5]
      \item Follow path for point: [1, 0, 1.5]
      \item Follow path for point: [1, 0, 10]
      \item Follow path for point: [1, -5, 10]
      \item Follow path for point: [0, -5, 1.5]
      \item Follow path for point: [0, -5, 1]
    \end{enumerate}

    The python code for this mission can be found in the appendix \ref{app1:follow_path_mission}

  \subsection{Behavior Go To Point in Occupancy Grid} \label{ch_5:subsect:behav_gtp_mission}

    This is the centre key of the navigation system, providing the most rich, autonomous behavior. The mission is as simplified version of the one made for the real boiler inspection, but fitted to the real flight scenario at hand. It's outline is presented below:

    \begin{enumerate}
      \item Take Off
      \item Go to $1.5$ meters height ([0, 0, 1.5])
      \item Go to $1$ meter to the front, maintaining the altitude. ([1, 0, 1.5])
      \item Go to $10$ meters height, maintaining the same distance to the wall. ([1, 0, 10])
      \item Go to $5$ meters to the right, keeping the same distance and altitude. ([1, -5, 10])
      \item Go to $1$ meter away from the wall, maintaining the same altitude. ([0, -5, 1.5])
      \item Go to $1$ meter height, maintaining the same distance to the wall. ([0, -5, 1])
      \item Land
    \end{enumerate}

    Please, find all the python code for this mission attached in the appendix \ref{app1:go_to_point_mission}.

  \subsection{Simulation} \label{ch_5:subsect:exp_simulation}

    \begin{figure}[!h]
      \centering
      \includegraphics[width=0.9\textwidth,height=0.4\textheight,keepaspectratio]{./Figures/BoilerSim.png}
      \caption{Simulated boiler. $57$ meters tall by $16$ meters (square section). The UAV is enclosed in the yellow circle.}
      \label{ch_5:fig:boiler_sim}
    \end{figure}

    For the simulation we created a blender model of a boiler and exported it to Gazebo, then we created a RotorS enabled Gazebo world with the hummingbird drone model. The dimensions of the simulated boiler are listed in table \ref{ch_5:table:boiler_sim_dims}. To get an idea of the proportions of the boiler with the aircraft see figure \ref{ch_5:fig:boiler_sim}. The drone has been outlined in yellow, it almost unseeable.

    \begin{table}[!h]
      \centering
      \begin{tabular}{lr} \toprule
        \multicolumn{2}{c}{\textit{Simulated boiler dimensions}}        \\ \midrule
        Width $\times$ Depth $\times$ Height & $16 \times 16 \times 57$ \\ \bottomrule
        \hline
      \end{tabular}
      \caption{Simulation computer specifications}
      \label{ch_5:table:boiler_sim_dims}
    \end{table}

    The specifications of the computer employed for all the simulations are depicted in the following table \ref{ch_5:table:laptop_specs}:

    \begin{table}[!h]
      \centering
      \begin{tabular}{lr} \toprule
        \multicolumn{1}{c}{\textit{component}} & \multicolumn{1}{c}{\textit{value}}   \\ \midrule
        Ram           & DDR 4 32 GB     \\
        Processor     & 3.4 Ghz 8 cores \\
        GPU           & GTX 1050 Ti     \\ \bottomrule
        \hline
      \end{tabular}
      \caption{Simulation computer specifications}
      \label{ch_5:table:laptop_specs}
    \end{table}

    In this scenario, performance does not comprise a problem because with enough GPU, the simulation can run smoothly and all processes can run with good memory support.

    Figure \ref{ch_5:fig:full_sim} shows the simulated world running inside gazebo, along with the camera output and a visualization of the current lidar map.
    
    \begin{figure}
      \centering
      \includegraphics[width=0.9\textwidth,height=0.5\textheight,keepaspectratio]{./Figures/FullSim.png}
      \caption{Gazebo and Rviz. Simulation visualization during the execution of the mission. Left: Gazebo World, top-right: Rviz lidar measures, bottom-right: Hummingbird front camera}
      \label{ch_5:fig:full_sim}
    \end{figure}

  \subsection{Real Flight} \label{ch_5:subsect:exp_real_flight}

    For the real flight tests we used the sports centre in the School of Industrial Engineers, which is a closed space that can serve for our purposes, it's dimensions are listed in table \ref{ch_5:table:sports_dims}

    \begin{table}[!h]
      \centering
      \begin{tabular}{lr} \toprule
        \multicolumn{2}{c}{\textit{Sports Centre Dimensions}}        \\ \midrule
        Width $\times$ Depth $\times$ Height & $10 \times 25 \times 14$ \\ \bottomrule
        \hline
      \end{tabular}
      \caption{Dimensions of the sports centre used for real flight tests. School of Industrial Engineers}
      \label{ch_5:table:sports_dims}
    \end{table}

    The chosen drone ships a DJI Manifold micro computer for onboard computation (\cite{dji_manifold_web}). Its technical details are contained in table \ref{ch_5:table:manifold_specs}

    \begin{table}[!h]
      \centering
      \begin{tabular}{lr} \toprule
        \multicolumn{1}{c}{\textit{component}} & \multicolumn{1}{c}{\textit{value}}   \\ \midrule
        Ram           & DDR 3 2 GB     \\
        Processor     & 2.5 Ghz 4 cores \\
        GPU           & NVIDIA Kepler GeForce \\ \bottomrule
        \hline
      \end{tabular}
      \caption{Onboard computer specification}
      \label{ch_5:table:manifold_specs}
    \end{table}

    Onboard computation is the better option in this case because there is no \textit{auto pilot} (fallback driver controller, shipped in many drones to do automatic hover when no orders are received) and given the distances it can travel and the altitude, ensuring WiFi coverage is difficult. Hence, the most secure option is to load all the necessary software inside the onboard computer and send just a few orders from the ground control station. More specifically, launch the Aerostack and the python mission.

    The manifold computer is not very powerful so special attention must be put on performance , if computing power drains it can be catastrophic. To aid in this situation a human pilot was prepared to take control during all the tests, although at the end it was not necessary.

\section{Experimental Results} \label{ch_5:sect:results}

  This section will explain the results obtained, we will first go through the simulation results and continue with the real flight ones. For each behavior, tables [ref tables] show: the correct execution of the behavior, the mean and standard deviation execution time for all points and the total time of the mission, also, the last row shows the averaged scores and times. Everything is measured in minutes and each experiment is run ten times, also, the timeout counter for each behavior is set to 4 minutes.
 
  \input{./Tables/Behavior_Generate_Path_results.tex}

  Table \ref{ch_5:table:generate_path_results} shows the results for the behavior \textit{generate path in occupancy grid}. Since this behavior does not perform any motion, it is just planning, a $100\%$ hits seems reasonable, it means that the planner does its job correctly, both the move base planner and the path planner modules work correctly, also, the behavior works as expected, generating all the requested paths. Furthermore, the times employed to generate the trajectories are very stable, which meets our requirements. 

  \input{./Tables/Behavior_Follow_Path_results.tex}

  This behavior had the worst performance of all, although it scores for 65\% of the points, it has a lot of variability, this may be due to the fact that it executes the motions blindly, no logic is applied to recalculate paths when needed. It just executes the given trajectory. This behavior is slighty more complicated than the previous one in the sense that it has to execute motions, which increases the difficulty.

  We acknowledged various fault causes, the very first one is estimation, although some points where not perfectly matched, the end positions where very close to the goal. This drift in localization arises from the lack of fine tuning, the localization EKF is not completely tuned for simulatation or the UAV. Also, during the tests we observed that the trajectory controller has some weird behaviors. There were various cases where the orders took no effect. Taking a look over the memory consumption clarified the assumptions that there are some conditions that cause the controller to hang in an intensive loop. As of the time of writing, the controller is being remade from the ground up.

  Not matching the target points means that the behavior does not finish and it only stops when the timeout is reached. This faulty finish condition, in turn adds up to the execution time, which explains some of the large times of execution. Tests 2, 4, 5 and 9 accounts for this fact, they score the least but also last the most. Interestingly enough, this test has more fully completed missions that the one of \textit{go to point}, 2 out of 10 versus 1 out time, but in general has more faulty points, rising the global time of execution for the whole test.

  \input{./Tables/Behavior_Go_To_Point_results.tex}

  This behavior is most complex, but also more stable than the previous one, it contains some logic to handle obstacles and track the drone position. Nevertheless, the bad matching of points is a problem, this is the case of the 2nd, 3rd and 5th tests, which fail to reach half of the points, rising the execution time to 15 minutes at worst, the correlation is clear. We strengthen our assumption with tests 8th and 9th, that amounts for 5 out of 6 and 6 out 6 goals reached, respectively, with the lowest times of all the tests. The implemented behavior works correctly in 68\% of the cases, which is not bad given the complexity in coordination needed to accomplish the task and the short time employed implementing it.

  The last test in our experiments are run in real flight, with the previously explained drone and environment. Due to time constraints and both hardware and pilot availability, we ran only 3 tests, the results are reported in table \ref{ch_5:table:real_flight_results}

  \input{./Tables/Real_flight_results.tex}

  It is worth noting that these tests are much more robust and fast, the intuition behind these results is that the localization module is widely tested and tuned for this specific aircraft which makes sense given that it is used for industrial level real flight missions. Also, the speed of this drone is comparably higher.

  Although these tests were run with a human operator fallback, just in case something went wrong, none of the tests required it. Even in the first test, where two points failed to match, the system recovered and successfully finished the mission. Furthermore, the two failed points where encountered when reaching the highest altitude and when moving away from the wall at highest altitude, points for and six, respectively (refer to \ref{ch_5:subsect:behav_gtp_mission} for detailed coordinates). Our best guess for this performance is based on the environment: the sports centre walls are made of various materials, it has about 8-9 meters of solid wall and then a grid gate with holes to the roof. We believe that the holes in the grid gate tricked the lidar based localization, which in turn caused the go to point not to match the target point. Also, as the localization is made out of the fusion of various sensor inputs, this phenomenon does not happen always, which is another proof of the robustness of the system.

  \textbf{[ToDo := Add speeds of each drone?]}

\section{Discussion} \label{ch_5:sect:discussion}

  The results obtained shed some light over the problems that arise in an autonomous navigation system, clarifying the need for more tests and a better prepared simulation environment.

  First, the environment used for simulation is very well suited for our needs, but lacks some tuning. The fact that there exists such difference between  the real flight mission and the simulated one is not good, although clearly improvable through extensing tuning and testing. Even though, the proposed system works reasonably well for the time invested to implementing and also highlights that we are on the correct track for future experiments.

This chapter presented all the validation missions and environment employed for testing all the implemented behaviors, also the results were presented and analyzed for discussion. The next chapter will conclude the thesis adding more in depth thoughts about the results obtained and further research.
